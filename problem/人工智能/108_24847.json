{"success":true,"data":{"id":24847,"name":"<p>请说明Transformer自注意力的计算过程，解释softmax进行缩放的原因以及不缩放会导致的结果</p>","options":null,"type":6,"level":2,"freq":0.002908184,"group_id":108,"kps":["深度学习","神经网络"],"years":[2025,2024,2023],"corps":["网易有道","字节跳动","快手","腾讯","商汤科技"],"emptyReason":"OverLimit","answer":" ","analysis":" ","more_ask":" ","mindmap":null,"keynote":" "}}