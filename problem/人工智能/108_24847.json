{
  "success": true,
  "data": {
    "id": 24847,
    "name": "<p>请说明Transformer自注意力的计算过程，解释softmax进行缩放的原因以及不缩放会导致的结果</p>",
    "options": null,
    "answer": "<h3>Transformer自注意力的计算过程</h3>\n<p>Transformer自注意力机制允许模型在处理序列时，对序列中的不同位置进行加权关注，从而捕捉序列内的依赖关系。其计算过程主要分为以下几个步骤：</p>\n<h4>1. 生成查询（Query）、键（Key）和值（Value）向量</h4>\n<p>对于输入序列 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>X</mi>\n          <mo>=</mo>\n          <mo stretchy=\"false\">[</mo>\n          <msub>\n            <mi>x</mi>\n            <mn>1</mn>\n          </msub>\n          <mo separator=\"true\">,</mo>\n          <msub>\n            <mi>x</mi>\n            <mn>2</mn>\n          </msub>\n          <mo separator=\"true\">,</mo>\n          <mi mathvariant=\"normal\">.</mi>\n          <mi mathvariant=\"normal\">.</mi>\n          <mi mathvariant=\"normal\">.</mi>\n          <mo separator=\"true\">,</mo>\n          <msub>\n            <mi>x</mi>\n            <mi>n</mi>\n          </msub>\n          <mo stretchy=\"false\">]</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">X = [x_1, x_2, ..., x_n]</annotation>\n      </semantics>\n    </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>x</mi>\n            <mi>i</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">x_i</annotation>\n      </semantics>\n    </math></span> 是序列中第 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>i</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">i</annotation>\n      </semantics>\n    </math></span> 个位置的输入向量。通过三个不同的线性变换矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msup>\n            <mi>W</mi>\n            <mi>Q</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">W^Q</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msup>\n            <mi>W</mi>\n            <mi>K</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">W^K</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msup>\n            <mi>W</mi>\n            <mi>V</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">W^V</annotation>\n      </semantics>\n    </math></span> 分别将输入向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>X</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">X</annotation>\n      </semantics>\n    </math></span> 映射为查询向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span>、键向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 和值向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">V</annotation>\n      </semantics>\n    </math></span>：</p>\n<ul>\n  <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msup>\n              <mi>W</mi>\n              <mi>Q</mi>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q = XW^Q</annotation>\n        </semantics>\n      </math></span></li>\n  <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msup>\n              <mi>W</mi>\n              <mi>K</mi>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K = XW^K</annotation>\n        </semantics>\n      </math></span></li>\n  <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>V</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msup>\n              <mi>W</mi>\n              <mi>V</mi>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">V = XW^V</annotation>\n        </semantics>\n      </math></span></li>\n</ul>\n<p>这里的 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">V</annotation>\n      </semantics>\n    </math></span> 都是矩阵，其形状分别为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mo stretchy=\"false\">[</mo>\n          <mi>n</mi>\n          <mo separator=\"true\">,</mo>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n          <mo stretchy=\"false\">]</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">[n, d_k]</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mo stretchy=\"false\">[</mo>\n          <mi>n</mi>\n          <mo separator=\"true\">,</mo>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n          <mo stretchy=\"false\">]</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">[n, d_k]</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mo stretchy=\"false\">[</mo>\n          <mi>n</mi>\n          <mo separator=\"true\">,</mo>\n          <msub>\n            <mi>d</mi>\n            <mi>v</mi>\n          </msub>\n          <mo stretchy=\"false\">]</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">[n, d_v]</annotation>\n      </semantics>\n    </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>n</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">n</annotation>\n      </semantics>\n    </math></span> 是序列的长度，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_k</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>v</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_v</annotation>\n      </semantics>\n    </math></span> 分别是查询/键向量和值向量的维度。</p>\n<h4>2. 计算注意力分数</h4>\n<p>\n  通过计算查询向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和键向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 的点积，得到注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>S</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S</annotation>\n      </semantics>\n    </math></span>：\n  <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>S</mi>\n          <mo>=</mo>\n          <mi>Q</mi>\n          <msup>\n            <mi>K</mi>\n            <mi>T</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S = QK^T</annotation>\n      </semantics>\n    </math></span>\n</p>\n<p>矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>S</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S</annotation>\n      </semantics>\n    </math></span> 的形状为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mo stretchy=\"false\">[</mo>\n          <mi>n</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>n</mi>\n          <mo stretchy=\"false\">]</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">[n, n]</annotation>\n      </semantics>\n    </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>S</mi>\n            <mrow>\n              <mi>i</mi>\n              <mi>j</mi>\n            </mrow>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S_{ij}</annotation>\n      </semantics>\n    </math></span> 表示第 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>i</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">i</annotation>\n      </semantics>\n    </math></span> 个位置的查询向量与第 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>j</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">j</annotation>\n      </semantics>\n    </math></span> 个位置的键向量的相似度。</p>\n<h4>3. 缩放注意力分数</h4>\n<p>\n  为了避免点积结果过大，对注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>S</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S</annotation>\n      </semantics>\n    </math></span> 进行缩放，缩放因子为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msqrt>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </msqrt>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n      </semantics>\n    </math></span>：\n  <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mover accent=\"true\">\n            <mi>S</mi>\n            <mo>^</mo>\n          </mover>\n          <mo>=</mo>\n          <mfrac>\n            <mi>S</mi>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mfrac>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\hat{S} = \\frac{S}{\\sqrt{d_k}}</annotation>\n      </semantics>\n    </math></span>\n</p>\n<h4>4. 应用Softmax函数</h4>\n<p>\n  对缩放后的注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mover accent=\"true\">\n            <mi>S</mi>\n            <mo>^</mo>\n          </mover>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\hat{S}</annotation>\n      </semantics>\n    </math></span> 应用Softmax函数，得到注意力权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">A</annotation>\n      </semantics>\n    </math></span>：\n  <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n          <mo>=</mo>\n          <mtext>softmax</mtext>\n          <mo stretchy=\"false\">(</mo>\n          <mover accent=\"true\">\n            <mi>S</mi>\n            <mo>^</mo>\n          </mover>\n          <mo stretchy=\"false\">)</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">A = \\text{softmax}(\\hat{S})</annotation>\n      </semantics>\n    </math></span>\n</p>\n<p>Softmax函数将注意力分数转换为概率分布，使得每一行的元素之和为 1。</p>\n<h4>5. 计算加权和</h4>\n<p>\n  将注意力权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">A</annotation>\n      </semantics>\n    </math></span> 与值向量矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">V</annotation>\n      </semantics>\n    </math></span> 相乘，得到自注意力机制的输出 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Z</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Z</annotation>\n      </semantics>\n    </math></span>：\n  <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Z</mi>\n          <mo>=</mo>\n          <mi>A</mi>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Z = AV</annotation>\n      </semantics>\n    </math></span>\n</p>\n<p>矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Z</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Z</annotation>\n      </semantics>\n    </math></span> 的形状为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mo stretchy=\"false\">[</mo>\n          <mi>n</mi>\n          <mo separator=\"true\">,</mo>\n          <msub>\n            <mi>d</mi>\n            <mi>v</mi>\n          </msub>\n          <mo stretchy=\"false\">]</mo>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">[n, d_v]</annotation>\n      </semantics>\n    </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>Z</mi>\n            <mi>i</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Z_i</annotation>\n      </semantics>\n    </math></span> 是第 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>i</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">i</annotation>\n      </semantics>\n    </math></span> 个位置的输出向量，它是值向量的加权和，权重由注意力权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">A</annotation>\n      </semantics>\n    </math></span> 的第 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>i</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">i</annotation>\n      </semantics>\n    </math></span> 行决定。</p>\n<h3>Softmax进行缩放的原因</h3>\n<p>在计算注意力分数时，查询向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和键向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 的点积可能会导致数值过大。具体来说，当 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_k</annotation>\n      </semantics>\n    </math></span> 较大时，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 的点积结果的方差也会增大。这是因为点积是向量元素的逐元素相乘并求和，向量维度越高，点积结果的波动就越大。</p>\n<p>Softmax函数对输入的数值非常敏感，当输入的数值差异较大时，Softmax函数会将大部分概率分配给数值最大的元素，而其他元素的概率几乎为 0。这种情况下，注意力权重会变得非常尖锐，模型只能关注到少数几个位置，而忽略了其他位置的信息，导致模型的表达能力下降。</p>\n<p>通过将注意力分数除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msqrt>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </msqrt>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n      </semantics>\n    </math></span>，可以将点积结果的方差缩小，使得Softmax函数的输入更加稳定，从而得到更加平滑的注意力权重分布。</p>\n<h3>不缩放会导致的结果</h3>\n<p>如果不进行缩放，注意力分数的数值可能会变得非常大，这会导致以下问题：</p>\n<h4>1. Softmax函数梯度消失</h4>\n<p>当注意力分数的数值非常大时，Softmax函数的输出会趋近于一个one - hot向量，即只有一个元素的概率接近 1，其他元素的概率接近 0。在这种情况下，Softmax函数的梯度会变得非常小，导致梯度消失问题。梯度消失会使得模型在训练过程中难以更新参数，从而影响模型的收敛速度和性能。</p>\n<h4>2. 注意力分布过于集中</h4>\n<p>不缩放的注意力分数会使得Softmax函数的输出过于集中在少数几个位置上，导致注意力分布变得非常尖锐。这意味着模型只能关注到序列中的少数几个位置，而忽略了其他位置的信息，从而降低了模型捕捉序列中长距离依赖关系的能力。</p>\n<p>综上所述，对注意力分数进行缩放是Transformer自注意力机制中非常重要的一步，它可以提高模型的稳定性和表达能力。</p>",
    "type": 6,
    "level": 2,
    "freq": 0.002908184,
    "analysis": "<h3>1. 题目核心</h3>\n<ul>\n  <li><strong>问题</strong>：说明Transformer自注意力的计算过程，解释softmax进行缩放的原因以及不缩放会导致的结果。</li>\n  <li><strong>考察点</strong>：\n    <ul>\n      <li>对Transformer自注意力机制计算流程的掌握。</li>\n      <li>理解softmax缩放的原理。</li>\n      <li>清楚不进行softmax缩放会产生的后果。</li>\n    </ul>\n  </li>\n</ul>\n<h3>2. 背景知识</h3>\n<h4>（1）Transformer架构</h4>\n<p>Transformer是一种基于注意力机制的深度学习模型，自注意力机制是其核心组件，能让模型在处理序列时关注序列中不同位置的信息。</p>\n<h4>（2）Softmax函数</h4>\n<p>Softmax函数用于将一组实数转换为概率分布，常用于多分类问题，在自注意力机制中用于计算注意力权重。</p>\n<h3>3. 解析</h3>\n<h4>（1）Transformer自注意力的计算过程</h4>\n<ul>\n  <li><strong>输入嵌入</strong>：将输入序列的每个元素转换为对应的嵌入向量，得到输入矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>X</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">X</annotation>\n        </semantics>\n      </math></span>，形状为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo separator=\"true\">,</mo>\n            <mi>d</mi>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">(n, d)</annotation>\n        </semantics>\n      </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>n</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">n</annotation>\n        </semantics>\n      </math></span> 是序列长度，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>d</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d</annotation>\n        </semantics>\n      </math></span> 是嵌入维度。</li>\n  <li><strong>生成Q、K、V矩阵</strong>：通过三个不同的线性变换矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>Q</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_Q</annotation>\n        </semantics>\n      </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>K</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_K</annotation>\n        </semantics>\n      </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>V</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_V</annotation>\n        </semantics>\n      </math></span> 分别对输入矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>X</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">X</annotation>\n        </semantics>\n      </math></span> 进行线性变换，得到查询矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>Q</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q = XW_Q</annotation>\n        </semantics>\n      </math></span>、键矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>K</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K = XW_K</annotation>\n        </semantics>\n      </math></span> 和值矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>V</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>V</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">V = XW_V</annotation>\n        </semantics>\n      </math></span>，形状均为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo separator=\"true\">,</mo>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">(n, d_k)</annotation>\n        </semantics>\n      </math></span> （<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_k</annotation>\n        </semantics>\n      </math></span> 是键和查询的维度）。</li>\n  <li><strong>计算注意力分数</strong>：计算查询矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q</annotation>\n        </semantics>\n      </math></span> 和键矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K</annotation>\n        </semantics>\n      </math></span> 的点积，得到注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>S</mi>\n            <mo>=</mo>\n            <mi>Q</mi>\n            <msup>\n              <mi>K</mi>\n              <mi>T</mi>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">S = QK^T</annotation>\n        </semantics>\n      </math></span>，形状为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo separator=\"true\">,</mo>\n            <mi>n</mi>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">(n, n)</annotation>\n        </semantics>\n      </math></span>。</li>\n  <li><strong>缩放注意力分数</strong>：将注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>S</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">S</annotation>\n        </semantics>\n      </math></span> 除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n        </semantics>\n      </math></span>，得到缩放后的注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msup>\n              <mi>S</mi>\n              <mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo>\n            </msup>\n            <mo>=</mo>\n            <mfrac>\n              <mi>S</mi>\n              <msqrt>\n                <msub>\n                  <mi>d</mi>\n                  <mi>k</mi>\n                </msub>\n              </msqrt>\n            </mfrac>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">S'=\\frac{S}{\\sqrt{d_k}}</annotation>\n        </semantics>\n      </math></span>。</li>\n  <li><strong>应用Softmax函数</strong>：对缩放后的注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msup>\n              <mi>S</mi>\n              <mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">S'</annotation>\n        </semantics>\n      </math></span> 应用Softmax函数，得到注意力权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>A</mi>\n            <mo>=</mo>\n            <mi>s</mi>\n            <mi>o</mi>\n            <mi>f</mi>\n            <mi>t</mi>\n            <mi>m</mi>\n            <mi>a</mi>\n            <mi>x</mi>\n            <mo stretchy=\"false\">(</mo>\n            <msup>\n              <mi>S</mi>\n              <mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo>\n            </msup>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">A = softmax(S')</annotation>\n        </semantics>\n      </math></span>，形状为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo separator=\"true\">,</mo>\n            <mi>n</mi>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">(n, n)</annotation>\n        </semantics>\n      </math></span>，且每一行元素之和为 1。</li>\n  <li><strong>计算输出</strong>：将注意力权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>A</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">A</annotation>\n        </semantics>\n      </math></span> 与值矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>V</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">V</annotation>\n        </semantics>\n      </math></span> 相乘，得到自注意力机制的输出矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>O</mi>\n            <mo>=</mo>\n            <mi>A</mi>\n            <mi>V</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">O = AV</annotation>\n        </semantics>\n      </math></span>，形状为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo separator=\"true\">,</mo>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">(n, d_k)</annotation>\n        </semantics>\n      </math></span>。</li>\n</ul>\n<h4>（2）softmax进行缩放的原因</h4>\n<ul>\n  <li><strong>避免梯度消失或爆炸</strong>：当 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_k</annotation>\n        </semantics>\n      </math></span> 较大时，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q</annotation>\n        </semantics>\n      </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K</annotation>\n        </semantics>\n      </math></span> 的点积结果会随着维度的增加而增大，导致softmax函数的输入值变得非常大。softmax函数在输入值较大时，其梯度会变得非常小，从而导致梯度消失问题。通过除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n        </semantics>\n      </math></span> 可以将点积结果缩小，避免梯度消失。</li>\n  <li><strong>使注意力分布更平滑</strong>：缩放操作可以使注意力分数的分布更加均匀，避免某些位置的注意力权重过大，从而使模型能够更全面地关注序列中的不同位置。</li>\n</ul>\n<h4>（3）不缩放会导致的结果</h4>\n<ul>\n  <li><strong>梯度消失</strong>：由于点积结果可能会变得非常大，softmax函数的梯度会趋近于 0，使得模型在训练过程中难以更新参数，导致训练速度变慢甚至无法收敛。</li>\n  <li><strong>注意力分布不均衡</strong>：不缩放会使某些位置的注意力分数远大于其他位置，经过softmax函数后，这些位置的注意力权重会趋近于 1，而其他位置的注意力权重会趋近于 0。这会导致模型只关注序列中的少数位置，忽略了其他重要信息，降低了模型的性能。</li>\n</ul>\n<h3>4. 示例代码（使用Python和PyTorch）</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn.functional as F\n\ndef self_attention(X, d_k):\n    # 假设已经有了W_Q, W_K, W_V\n    W_Q = torch.randn(X.size(1), d_k)\n    W_K = torch.randn(X.size(1), d_k)\n    W_V = torch.randn(X.size(1), d_k)\n\n    Q = torch.matmul(X, W_Q)\n    K = torch.matmul(X, W_K)\n    V = torch.matmul(X, W_V)\n\n    # 计算注意力分数\n    S = torch.matmul(Q, K.transpose(-2, -1))\n\n    # 缩放注意力分数\n    S_scaled = S / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n\n    # 应用Softmax函数\n    A = F.softmax(S_scaled, dim=-1)\n\n    # 计算输出\n    O = torch.matmul(A, V)\n    return O\n\n# 示例输入\nn = 5  # 序列长度\nd = 10  # 嵌入维度\nd_k = 8  # 键和查询的维度\nX = torch.randn(n, d)\n\noutput = self_attention(X, d_k)\nprint(output.shape)\n</code></pre>\n<h3>5. 常见误区</h3>\n<h4>（1）忽略缩放的重要性</h4>\n<ul>\n  <li>误区：认为不进行缩放对模型性能影响不大。</li>\n  <li>纠正：不缩放会导致梯度消失和注意力分布不均衡，严重影响模型的训练和性能。</li>\n</ul>\n<h4>（2）错误理解缩放的原理</h4>\n<ul>\n  <li>误区：不清楚为什么要除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n        </semantics>\n      </math></span>。</li>\n  <li>纠正：除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n        </semantics>\n      </math></span> 是为了避免梯度消失和使注意力分布更平滑。</li>\n</ul>\n<h3>6. 总结回答</h3>\n<p>Transformer自注意力的计算过程如下：首先将输入序列转换为嵌入矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>X</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">X</annotation>\n      </semantics>\n    </math></span>，然后通过线性变换得到查询矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span>、键矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 和值矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">V</annotation>\n      </semantics>\n    </math></span>，计算 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 的点积得到注意力分数矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>S</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S</annotation>\n      </semantics>\n    </math></span>，将 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>S</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">S</annotation>\n      </semantics>\n    </math></span> 除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msqrt>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </msqrt>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n      </semantics>\n    </math></span> 进行缩放，再对缩放后的矩阵应用Softmax函数得到注意力权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">A</annotation>\n      </semantics>\n    </math></span>，最后将 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">A</annotation>\n      </semantics>\n    </math></span> 与 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">V</annotation>\n      </semantics>\n    </math></span> 相乘得到输出矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>O</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">O</annotation>\n      </semantics>\n    </math></span>。</p>\n<p>softmax进行缩放的原因主要有两个：一是避免梯度消失或爆炸，当 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_k</annotation>\n      </semantics>\n    </math></span> 较大时，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 的点积结果会增大，导致softmax函数梯度变小，缩放可以避免这种情况；二是使注意力分布更平滑，让模型能更全面地关注序列中的不同位置。</p>\n<p>如果不进行缩放，会导致梯度消失，使模型训练速度变慢甚至无法收敛，同时会使注意力分布不均衡，模型只关注序列中的少数位置，忽略其他重要信息，降低模型性能。</p>",
    "more_ask": "<ol>\n  <li>\n    <p>\n      请详细说明在多头自注意力机制中，不同头是如何协同工作以捕捉不同特征的？\n      提示：思考不同头的参数设置、输入输出以及它们在整体模型中的作用。\n    </p>\n  </li>\n  <li>\n    <p>\n      自注意力机制中的掩码（masking）操作有哪些类型，分别在什么场景下使用？\n      提示：考虑训练和推理阶段的不同需求，以及不同类型的序列任务。\n    </p>\n  </li>\n  <li>\n    <p>\n      当输入序列长度非常长时，自注意力机制会面临什么挑战，有哪些改进方法？\n      提示：从计算复杂度、内存占用等方面思考挑战，再考虑现有的优化算法。\n    </p>\n  </li>\n  <li>\n    <p>\n      请解释自注意力机制中的键（key）、查询（query）和值（value）矩阵是如何从输入中生成的，以及它们的物理意义。\n      提示：结合线性变换和输入特征的表示来解释。\n    </p>\n  </li>\n  <li>\n    <p>\n      在自注意力计算中，除了softmax缩放，还有其他的归一化方法可以使用吗，它们的优缺点是什么？\n      提示：联想常见的归一化技术，并对比它们在自注意力机制中的适用性。\n    </p>\n  </li>\n</ol>",
    "mindmap": "mindmap\n  root((Transformer自注意力计算过程))\n    自注意力计算步骤\n      生成Q、K、V向量\n        输入序列X经线性变换矩阵映射\n        Q = XW^Q\n        K = XW^K\n        V = XW^V\n      计算注意力分数\n        S = QK^T\n      缩放注意力分数\n        缩放因子为√d_k\n        Ŝ = S/√d_k\n      应用Softmax函数\n        A = softmax(Ŝ)\n      计算加权和\n        Z = AV\n    Softmax缩放原因\n      点积结果数值可能过大\n      避免注意力权重尖锐\n      使Softmax输入更稳定\n    不缩放导致的结果\n      Softmax函数梯度消失\n      注意力分布过于集中",
    "keynote": "Transformer自注意力计算步骤：\n1. 生成Q、K、V向量：输入序列X经线性变换矩阵W^Q、W^K、W^V映射得到Q、K、V\n2. 计算注意力分数：S = QK^T\n3. 缩放注意力分数：缩放因子√d_k，Ŝ = S/√d_k\n4. 应用Softmax函数：A = softmax(Ŝ)\n5. 计算加权和：Z = AV\n\nSoftmax缩放原因：避免点积结果数值过大，使注意力权重更平滑\n\n不缩放结果：Softmax梯度消失，注意力分布集中",
    "group_id": 108,
    "kps": [
      "深度学习",
      "神经网络"
    ],
    "years": [
      2025,
      2024,
      2023
    ],
    "corps": [
      "网易有道",
      "字节跳动",
      "快手",
      "腾讯",
      "商汤科技"
    ]
  }
}