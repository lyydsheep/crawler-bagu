{
  "success": true,
  "data": {
    "id": 285830,
    "name": "<p>在Transformer中，为什么使用Layer Normalization（Ln）而不是Batch Normalization（BN）？请写出Attention的公式，说明Encoder和Decoder是如何交互的，以及Decoder有几层Multi-Head Attention（mha），并阐述Transformer和RNN的不同之处。</p>",
    "options": null,
    "type": 6,
    "level": 3,
    "freq": 0.0004154549,
    "group_id": 108,
    "kps": [
      "深度学习",
      "自然语言处理",
      "神经网络"
    ],
    "years": [
      2024
    ],
    "corps": [
      "腾讯"
    ],
    "emptyReason": "OverLimit",
    "answer": " ",
    "analysis": " ",
    "more_ask": " ",
    "mindmap": null,
    "keynote": " "
  }
}