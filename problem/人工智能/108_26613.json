{
  "success": true,
  "data": {
    "id": 26613,
    "name": "<p>请说明Transformer QKV的计算公式，以及为什么要除以√dk</p>",
    "options": null,
    "answer": "<h3>Transformer QKV计算公式</h3>\n<p>在Transformer架构里，Q（Query）、K（Key）、V（Value）是通过输入的词嵌入矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>X</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">X</annotation>\n      </semantics>\n    </math></span> 分别与三个不同的权重矩阵相乘得到的。假设输入的词嵌入矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>X</mi>\n          <mo>∈</mo>\n          <msup>\n            <mi mathvariant=\"double-struck\">R</mi>\n            <mrow>\n              <mi>n</mi>\n              <mo>×</mo>\n              <mi>d</mi>\n            </mrow>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">X \\in \\mathbb{R}^{n \\times d}</annotation>\n      </semantics>\n    </math></span>，这里的 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>n</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">n</annotation>\n      </semantics>\n    </math></span> 代表序列的长度，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>d</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d</annotation>\n      </semantics>\n    </math></span> 表示词嵌入的维度。</p>\n<ul>\n  <li><strong>Query矩阵</strong>：<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>Q</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q = XW_Q</annotation>\n        </semantics>\n      </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>Q</mi>\n            </msub>\n            <mo>∈</mo>\n            <msup>\n              <mi mathvariant=\"double-struck\">R</mi>\n              <mrow>\n                <mi>d</mi>\n                <mo>×</mo>\n                <msub>\n                  <mi>d</mi>\n                  <mi>k</mi>\n                </msub>\n              </mrow>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_Q \\in \\mathbb{R}^{d \\times d_k}</annotation>\n        </semantics>\n      </math></span> 是可学习的权重矩阵，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_k</annotation>\n        </semantics>\n      </math></span> 是Query和Key向量的维度，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n            <mo>∈</mo>\n            <msup>\n              <mi mathvariant=\"double-struck\">R</mi>\n              <mrow>\n                <mi>n</mi>\n                <mo>×</mo>\n                <msub>\n                  <mi>d</mi>\n                  <mi>k</mi>\n                </msub>\n              </mrow>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q \\in \\mathbb{R}^{n \\times d_k}</annotation>\n        </semantics>\n      </math></span>。</li>\n  <li><strong>Key矩阵</strong>：<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>K</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K = XW_K</annotation>\n        </semantics>\n      </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>K</mi>\n            </msub>\n            <mo>∈</mo>\n            <msup>\n              <mi mathvariant=\"double-struck\">R</mi>\n              <mrow>\n                <mi>d</mi>\n                <mo>×</mo>\n                <msub>\n                  <mi>d</mi>\n                  <mi>k</mi>\n                </msub>\n              </mrow>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_K \\in \\mathbb{R}^{d \\times d_k}</annotation>\n        </semantics>\n      </math></span> 是可学习的权重矩阵，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n            <mo>∈</mo>\n            <msup>\n              <mi mathvariant=\"double-struck\">R</mi>\n              <mrow>\n                <mi>n</mi>\n                <mo>×</mo>\n                <msub>\n                  <mi>d</mi>\n                  <mi>k</mi>\n                </msub>\n              </mrow>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K \\in \\mathbb{R}^{n \\times d_k}</annotation>\n        </semantics>\n      </math></span>。</li>\n  <li><strong>Value矩阵</strong>：<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>V</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>V</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">V = XW_V</annotation>\n        </semantics>\n      </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>V</mi>\n            </msub>\n            <mo>∈</mo>\n            <msup>\n              <mi mathvariant=\"double-struck\">R</mi>\n              <mrow>\n                <mi>d</mi>\n                <mo>×</mo>\n                <msub>\n                  <mi>d</mi>\n                  <mi>v</mi>\n                </msub>\n              </mrow>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_V \\in \\mathbb{R}^{d \\times d_v}</annotation>\n        </semantics>\n      </math></span> 是可学习的权重矩阵，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mi>v</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_v</annotation>\n        </semantics>\n      </math></span> 是Value向量的维度，通常 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mi>v</mi>\n            </msub>\n            <mo>=</mo>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_v = d_k</annotation>\n        </semantics>\n      </math></span>，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>V</mi>\n            <mo>∈</mo>\n            <msup>\n              <mi mathvariant=\"double-struck\">R</mi>\n              <mrow>\n                <mi>n</mi>\n                <mo>×</mo>\n                <msub>\n                  <mi>d</mi>\n                  <mi>v</mi>\n                </msub>\n              </mrow>\n            </msup>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">V \\in \\mathbb{R}^{n \\times d_v}</annotation>\n        </semantics>\n      </math></span>。</li>\n</ul>\n<p>在得到Q、K、V矩阵之后，会计算注意力分数，公式如下：</p>\n<p><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n          <mi>t</mi>\n          <mi>t</mi>\n          <mi>e</mi>\n          <mi>n</mi>\n          <mi>t</mi>\n          <mi>i</mi>\n          <mi>o</mi>\n          <mi>n</mi>\n          <mo stretchy=\"false\">(</mo>\n          <mi>Q</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>K</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>V</mi>\n          <mo stretchy=\"false\">)</mo>\n          <mo>=</mo>\n          <mtext>softmax</mtext>\n          <mo stretchy=\"false\">(</mo>\n          <mfrac>\n            <mrow>\n              <mi>Q</mi>\n              <msup>\n                <mi>K</mi>\n                <mi>T</mi>\n              </msup>\n            </mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mfrac>\n          <mo stretchy=\"false\">)</mo>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V</annotation>\n      </semantics>\n    </math></span></p>\n<h3>为什么要除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msqrt>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </msqrt>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n      </semantics>\n    </math></span></h3>\n<p>在计算注意力分数时，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n          <msup>\n            <mi>K</mi>\n            <mi>T</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">QK^T</annotation>\n      </semantics>\n    </math></span> 是Query矩阵和Key矩阵的转置相乘，得到的结果是一个 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>n</mi>\n          <mo>×</mo>\n          <mi>n</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">n \\times n</annotation>\n      </semantics>\n    </math></span> 的矩阵，矩阵中的每个元素代表了序列中不同位置之间的相似度。当 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_k</annotation>\n      </semantics>\n    </math></span> 较大时，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 向量的点积结果的方差会增大。</p>\n<p>假设 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 中的元素是独立同分布的随机变量，均值为 0，方差为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msup>\n            <mi>σ</mi>\n            <mn>2</mn>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sigma^2</annotation>\n      </semantics>\n    </math></span>。那么 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q</annotation>\n      </semantics>\n    </math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K</annotation>\n      </semantics>\n    </math></span> 向量点积的方差为：</p>\n<p><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mtext>Var</mtext>\n          <mo stretchy=\"false\">(</mo>\n          <mi>Q</mi>\n          <mo>⋅</mo>\n          <mi>K</mi>\n          <mo stretchy=\"false\">)</mo>\n          <mo>=</mo>\n          <msubsup>\n            <mo>∑</mo>\n            <mrow>\n              <mi>i</mi>\n              <mo>=</mo>\n              <mn>1</mn>\n            </mrow>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </msubsup>\n          <mtext>Var</mtext>\n          <mo stretchy=\"false\">(</mo>\n          <msub>\n            <mi>Q</mi>\n            <mi>i</mi>\n          </msub>\n          <msub>\n            <mi>K</mi>\n            <mi>i</mi>\n          </msub>\n          <mo stretchy=\"false\">)</mo>\n          <mo>=</mo>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n          <msup>\n            <mi>σ</mi>\n            <mn>2</mn>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\text{Var}(Q \\cdot K) = \\sum_{i = 1}^{d_k} \\text{Var}(Q_i K_i) = d_k \\sigma^2</annotation>\n      </semantics>\n    </math></span></p>\n<p>随着 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_k</annotation>\n      </semantics>\n    </math></span> 的增大，点积的方差也会线性增大。这会导致 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n          <msup>\n            <mi>K</mi>\n            <mi>T</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">QK^T</annotation>\n      </semantics>\n    </math></span> 的值变得很大，使得softmax函数的梯度变得很小。因为softmax函数在输入值较大时，会趋近于一个one - hot向量，梯度在这个区域非常小，从而造成梯度消失的问题，影响模型的训练效果。</p>\n<p>除以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msqrt>\n            <msub>\n              <mi>d</mi>\n              <mi>k</mi>\n            </msub>\n          </msqrt>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sqrt{d_k}</annotation>\n      </semantics>\n    </math></span> 可以将 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n          <msup>\n            <mi>K</mi>\n            <mi>T</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">QK^T</annotation>\n      </semantics>\n    </math></span> 的方差稳定在 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msup>\n            <mi>σ</mi>\n            <mn>2</mn>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">\\sigma^2</annotation>\n      </semantics>\n    </math></span>，避免了因 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_k</annotation>\n      </semantics>\n    </math></span> 过大导致的梯度消失问题，使得模型在训练过程中更加稳定，能够更有效地学习到序列中不同位置之间的依赖关系。</p>",
    "type": 6,
    "level": 2,
    "freq": 0.00249273,
    "analysis": "<h3>1. 题目核心</h3>\n<ul>\n  <li><strong>问题</strong>：说明Transformer QKV的计算公式，以及为何要除以√dk。</li>\n  <li><strong>考察点</strong>：\n    <ul>\n      <li>对Transformer中Q（查询）、K（键）、V（值）计算原理的理解。</li>\n      <li>对除以√dk这一操作目的的掌握。</li>\n    </ul>\n  </li>\n</ul>\n<h3>2. 背景知识</h3>\n<ul>\n  <li><strong>Transformer架构</strong>：是一种基于注意力机制的深度学习架构，在自然语言处理等领域有广泛应用。注意力机制允许模型在处理序列时，关注序列中不同位置的信息。</li>\n  <li><strong>QKV矩阵</strong>：在Transformer的注意力机制中，输入序列会通过线性变换得到Q、K、V三个矩阵，用于计算注意力分数。</li>\n</ul>\n<h3>3. 解析</h3>\n<h4>（1）QKV的计算公式</h4>\n<p>设输入序列为X，其维度为[seq_len, embedding_dim]。通过三个不同的线性变换矩阵Wq、Wk、Wv分别对输入X进行线性变换，得到Q、K、V矩阵。</p>\n<ul>\n  <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>Q</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>q</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">Q = XW_q</annotation>\n        </semantics>\n      </math></span></li>\n  <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>K</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>k</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">K = XW_k</annotation>\n        </semantics>\n      </math></span></li>\n  <li>\n    <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi>V</mi>\n            <mo>=</mo>\n            <mi>X</mi>\n            <msub>\n              <mi>W</mi>\n              <mi>v</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">V = XW_v</annotation>\n        </semantics>\n      </math></span>\n    其中，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>q</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_q</annotation>\n        </semantics>\n      </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>k</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_k</annotation>\n        </semantics>\n      </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>W</mi>\n              <mi>v</mi>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">W_v</annotation>\n        </semantics>\n      </math></span>分别是查询、键、值的权重矩阵，维度分别为[embedding_dim, dk]、[embedding_dim, dk]、[embedding_dim, dv]，dk是键向量的维度，dv是值向量的维度。\n  </li>\n</ul>\n<h4>（2）注意力分数的计算</h4>\n<p>\n  计算Q和K的点积，得到注意力分数矩阵，公式为：\n  <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n          <mi>t</mi>\n          <mi>t</mi>\n          <mi>e</mi>\n          <mi>n</mi>\n          <mi>t</mi>\n          <mi>i</mi>\n          <mi>o</mi>\n          <mi>n</mi>\n          <mo stretchy=\"false\">(</mo>\n          <mi>Q</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>K</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>V</mi>\n          <mo stretchy=\"false\">)</mo>\n          <mo>=</mo>\n          <mi>s</mi>\n          <mi>o</mi>\n          <mi>f</mi>\n          <mi>t</mi>\n          <mi>m</mi>\n          <mi>a</mi>\n          <mi>x</mi>\n          <mo stretchy=\"false\">(</mo>\n          <mfrac>\n            <mrow>\n              <mi>Q</mi>\n              <msup>\n                <mi>K</mi>\n                <mi>T</mi>\n              </msup>\n            </mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mfrac>\n          <mo stretchy=\"false\">)</mo>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V</annotation>\n      </semantics>\n    </math></span>\n  其中，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n          <msup>\n            <mi>K</mi>\n            <mi>T</mi>\n          </msup>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">QK^T</annotation>\n      </semantics>\n    </math></span>的维度是[seq_len, seq_len]，表示每个位置的查询向量与所有位置的键向量的相似度。\n</p>\n<h4>（3）为什么要除以√dk</h4>\n<ul>\n  <li><strong>避免点积结果过大</strong>：当dk较大时，Q和K的点积结果会变得很大。因为点积是向量元素对应相乘再求和，向量维度dk越大，点积结果的数值范围可能越大。</li>\n  <li><strong>防止softmax函数梯度消失</strong>：softmax函数在输入值较大时，其梯度会变得非常小，导致梯度消失问题。除以√dk可以将点积结果的数值范围缩小，使得softmax函数的输入更加合理，避免梯度消失，从而使模型训练更加稳定。</li>\n</ul>\n<h3>4. 示例代码（使用Python和PyTorch）</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\n# 输入序列\nseq_len = 5\nembedding_dim = 10\nX = torch.randn(seq_len, embedding_dim)\n\n# 定义权重矩阵\ndk = 6\ndv = 6\nWq = nn.Linear(embedding_dim, dk)\nWk = nn.Linear(embedding_dim, dk)\nWv = nn.Linear(embedding_dim, dv)\n\n# 计算Q、K、V\nQ = Wq(X)\nK = Wk(X)\nV = Wv(X)\n\n# 计算注意力分数\nattention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\nattention_probs = torch.softmax(attention_scores, dim=-1)\noutput = torch.matmul(attention_probs, V)\n\nprint(output.shape)\n</code></pre>\n<h3>5. 常见误区</h3>\n<h4>（1）忽略除以√dk的作用</h4>\n<ul>\n  <li>误区：只给出QKV的计算公式，没有解释除以√dk的原因。</li>\n  <li>纠正：详细说明除以√dk是为了避免点积结果过大，防止softmax函数梯度消失，保证模型训练的稳定性。</li>\n</ul>\n<h4>（2）错误理解QKV的计算过程</h4>\n<ul>\n  <li>误区：混淆Q、K、V的计算方式，或者不清楚权重矩阵的作用。</li>\n  <li>纠正：明确Q、K、V是通过输入序列X与不同的权重矩阵进行线性变换得到的。</li>\n</ul>\n<h3>6. 总结回答</h3>\n<p>“在Transformer中，Q、K、V的计算公式如下：设输入序列为X，通过三个不同的线性变换矩阵Wq、Wk、Wv分别对X进行线性变换，得到<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>Q</mi>\n          <mo>=</mo>\n          <mi>X</mi>\n          <msub>\n            <mi>W</mi>\n            <mi>q</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Q = XW_q</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>K</mi>\n          <mo>=</mo>\n          <mi>X</mi>\n          <msub>\n            <mi>W</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">K = XW_k</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>V</mi>\n          <mo>=</mo>\n          <mi>X</mi>\n          <msub>\n            <mi>W</mi>\n            <mi>v</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">V = XW_v</annotation>\n      </semantics>\n    </math></span>，其中<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>W</mi>\n            <mi>q</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">W_q</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>W</mi>\n            <mi>k</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">W_k</annotation>\n      </semantics>\n    </math></span>、<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>W</mi>\n            <mi>v</mi>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">W_v</annotation>\n      </semantics>\n    </math></span>分别是查询、键、值的权重矩阵。</p>\n<p>注意力分数的计算为<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <mi>A</mi>\n          <mi>t</mi>\n          <mi>t</mi>\n          <mi>e</mi>\n          <mi>n</mi>\n          <mi>t</mi>\n          <mi>i</mi>\n          <mi>o</mi>\n          <mi>n</mi>\n          <mo stretchy=\"false\">(</mo>\n          <mi>Q</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>K</mi>\n          <mo separator=\"true\">,</mo>\n          <mi>V</mi>\n          <mo stretchy=\"false\">)</mo>\n          <mo>=</mo>\n          <mi>s</mi>\n          <mi>o</mi>\n          <mi>f</mi>\n          <mi>t</mi>\n          <mi>m</mi>\n          <mi>a</mi>\n          <mi>x</mi>\n          <mo stretchy=\"false\">(</mo>\n          <mfrac>\n            <mrow>\n              <mi>Q</mi>\n              <msup>\n                <mi>K</mi>\n                <mi>T</mi>\n              </msup>\n            </mrow>\n            <msqrt>\n              <msub>\n                <mi>d</mi>\n                <mi>k</mi>\n              </msub>\n            </msqrt>\n          </mfrac>\n          <mo stretchy=\"false\">)</mo>\n          <mi>V</mi>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V</annotation>\n      </semantics>\n    </math></span>，这里除以√dk是为了避免当dk较大时，Q和K的点积结果过大。因为点积结果过大会导致softmax函数的输入值过大，进而使softmax函数的梯度变得非常小，出现梯度消失问题。除以√dk可以缩小点积结果的数值范围，使softmax函数的输入更加合理，保证模型训练的稳定性。”</p>",
    "more_ask": "<h3>1. 不同场景下QKV计算的调整</h3>\n<p>\n  问题：在处理长序列数据时，QKV的计算可能会面临哪些挑战，如何对QKV的计算公式进行调整以应对这些挑战？\n  提示：思考长序列带来的计算复杂度、内存占用等问题，以及一些针对长序列优化的方法，如稀疏注意力机制。\n</p>\n<h3>2. 除以√dk的理论依据拓展</h3>\n<p>\n  问题：除了从数学角度解释除以√dk的作用，从信息论的角度如何理解这一操作对模型性能的影响？\n  提示：考虑信息论中信息熵、互信息等概念，以及除以√dk如何影响Q和K之间的信息交互。\n</p>\n<h3>3. QKV计算中的矩阵运算细节</h3>\n<p>\n  问题：在实际编程实现QKV计算时，矩阵乘法的顺序会对计算效率产生影响吗？如何选择最优的矩阵乘法顺序？\n  提示：了解矩阵乘法的结合律，以及不同硬件（如GPU）对矩阵乘法顺序的偏好。\n</p>\n<h3>4. 不同类型注意力机制中QKV的变化</h3>\n<p>\n  问题：在多头注意力机制中，QKV的计算与普通注意力机制有什么不同？这种不同带来了什么优势？\n  提示：思考多头注意力机制中多个头的并行计算，以及如何通过不同的头捕捉不同的特征信息。\n</p>\n<h3>5. QKV计算对模型可解释性的影响</h3>\n<p>\n  问题：QKV的计算过程如何影响Transformer模型的可解释性？有没有方法可以提高基于QKV计算的模型的可解释性？\n  提示：考虑QKV计算中权重的分布、注意力分数的含义，以及一些可解释性分析方法，如注意力可视化。\n</p>",
    "mindmap": "mindmap\n  root((Transformer QKV计算公式))\n    QKV矩阵计算\n      输入词嵌入矩阵\n        维度：$X \\in \\mathbb{R}^{n \\times d}$\n        n：序列长度\n        d：词嵌入维度\n      Query矩阵\n        公式：$Q = XW_Q$\n        $W_Q$ 维度：$\\mathbb{R}^{d \\times d_k}$\n        Q维度：$\\mathbb{R}^{n \\times d_k}$\n      Key矩阵\n        公式：$K = XW_K$\n        $W_K$ 维度：$\\mathbb{R}^{d \\times d_k}$\n        K维度：$\\mathbb{R}^{n \\times d_k}$\n      Value矩阵\n        公式：$V = XW_V$\n        $W_V$ 维度：$\\mathbb{R}^{d \\times d_v}$\n        d_v：Value向量维度\n        V维度：$\\mathbb{R}^{n \\times d_v}$\n    注意力分数计算\n      公式：$Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n    除以 $\\sqrt{d_k}$ 的原因\n      $QK^T$ 结果\n        维度：$n \\times n$\n        元素含义：序列不同位置相似度\n      方差问题\n        点积方差：$\\text{Var}(Q \\cdot K) = d_k \\sigma^2$\n        影响：$d_k$ 增大，方差线性增大，导致梯度消失\n      作用\n        稳定方差：将 $QK^T$ 方差稳定在 $\\sigma^2$\n        效果：避免梯度消失，使模型训练更稳定",
    "keynote": "Transformer架构中QKV通过输入词嵌入矩阵X与不同权重矩阵相乘得到\n输入词嵌入矩阵 $X \\in \\mathbb{R}^{n \\times d}$，n为序列长度，d为词嵌入维度\nQuery矩阵 $Q = XW_Q$，$W_Q \\in \\mathbb{R}^{d \\times d_k}$，$Q \\in \\mathbb{R}^{n \\times d_k}$\nKey矩阵 $K = XW_K$，$W_K \\in \\mathbb{R}^{d \\times d_k}$，$K \\in \\mathbb{R}^{n \\times d_k}$\nValue矩阵 $V = XW_V$，$W_V \\in \\mathbb{R}^{d \\times d_v}$，$V \\in \\mathbb{R}^{n \\times d_v}$\n注意力分数公式 $Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n$QK^T$ 是 $n \\times n$ 矩阵，元素代表序列不同位置相似度\n$d_k$ 增大，$Q$ 和 $K$ 点积方差线性增大，导致梯度消失\n除以 $\\sqrt{d_k}$ 可稳定 $QK^T$ 方差，避免梯度消失，使模型训练更稳定",
    "group_id": 108,
    "kps": [
      "深度学习",
      "神经网络"
    ],
    "years": [
      2024,
      2023,
      2022
    ],
    "corps": [
      "济南众阳健康",
      "滴滴",
      "拼多多",
      "字节跳动",
      "美团",
      "58同城"
    ]
  }
}