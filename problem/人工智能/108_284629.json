{
  "success": true,
  "data": {
    "id": 284629,
    "name": "<p>Transformer前馈神经网络的结构是怎样的，为什么会先提升维度后缩小维度</p>",
    "options": null,
    "answer": "<h3>Transformer前馈神经网络的结构</h3>\n<p>Transformer中的前馈神经网络（Feed - Forward Neural Network，FFN）是一个两层的全连接网络，它独立地应用于序列中的每个位置。具体结构如下：</p>\n<ol>\n  <li><strong>第一层全连接层</strong>：输入向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi mathvariant=\"bold\">x</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation>\n        </semantics>\n      </math></span> 首先通过一个线性变换，即与权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">W</mi>\n              <mn>1</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{W}_1</annotation>\n        </semantics>\n      </math></span> 相乘并加上偏置向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">b</mi>\n              <mn>1</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{b}_1</annotation>\n        </semantics>\n      </math></span>。假设输入向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mi mathvariant=\"bold\">x</mi>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation>\n        </semantics>\n      </math></span> 的维度是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span>，权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">W</mi>\n              <mn>1</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{W}_1</annotation>\n        </semantics>\n      </math></span> 的维度是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n            <mo>×</mo>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model} \\times d_{ff}</annotation>\n        </semantics>\n      </math></span>，其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n        </semantics>\n      </math></span> 是前馈网络中间层的维度，通常 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n        </semantics>\n      </math></span> 大于 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span>。这一步的计算可以表示为：\n    <ul>\n      <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n            <semantics>\n              <mrow>\n                <msub>\n                  <mi mathvariant=\"bold\">z</mi>\n                  <mn>1</mn>\n                </msub>\n                <mo>=</mo>\n                <mtext>Linear</mtext>\n                <mo stretchy=\"false\">(</mo>\n                <mi mathvariant=\"bold\">x</mi>\n                <mo stretchy=\"false\">)</mo>\n                <mo>=</mo>\n                <mi mathvariant=\"bold\">x</mi>\n                <msub>\n                  <mi mathvariant=\"bold\">W</mi>\n                  <mn>1</mn>\n                </msub>\n                <mo>+</mo>\n                <msub>\n                  <mi mathvariant=\"bold\">b</mi>\n                  <mn>1</mn>\n                </msub>\n              </mrow>\n              <annotation encoding=\"application/x-tex\">\\mathbf{z}_1 = \\text{Linear}(\\mathbf{x})=\\mathbf{x}\\mathbf{W}_1+\\mathbf{b}_1</annotation>\n            </semantics>\n          </math></span></li>\n    </ul>\n  </li>\n  <li><strong>激活函数</strong>：经过第一层全连接层的输出 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">z</mi>\n              <mn>1</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{z}_1</annotation>\n        </semantics>\n      </math></span> 会通过一个非线性激活函数，在Transformer中通常使用的是ReLU（Rectified Linear Unit）函数。ReLU函数的定义为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <mtext>ReLU</mtext>\n            <mo stretchy=\"false\">(</mo>\n            <mi>z</mi>\n            <mo stretchy=\"false\">)</mo>\n            <mo>=</mo>\n            <mi>max</mi>\n            <mo>⁡</mo>\n            <mo stretchy=\"false\">(</mo>\n            <mn>0</mn>\n            <mo separator=\"true\">,</mo>\n            <mi>z</mi>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\text{ReLU}(z)=\\max(0, z)</annotation>\n        </semantics>\n      </math></span>。所以经过激活函数后的输出为：\n    <ul>\n      <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n            <semantics>\n              <mrow>\n                <msub>\n                  <mi mathvariant=\"bold\">z</mi>\n                  <mn>2</mn>\n                </msub>\n                <mo>=</mo>\n                <mtext>ReLU</mtext>\n                <mo stretchy=\"false\">(</mo>\n                <msub>\n                  <mi mathvariant=\"bold\">z</mi>\n                  <mn>1</mn>\n                </msub>\n                <mo stretchy=\"false\">)</mo>\n              </mrow>\n              <annotation encoding=\"application/x-tex\">\\mathbf{z}_2 = \\text{ReLU}(\\mathbf{z}_1)</annotation>\n            </semantics>\n          </math></span></li>\n    </ul>\n  </li>\n  <li><strong>第二层全连接层</strong>：经过激活函数处理后的向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">z</mi>\n              <mn>2</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{z}_2</annotation>\n        </semantics>\n      </math></span> 再通过第二个线性变换，与权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">W</mi>\n              <mn>2</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{W}_2</annotation>\n        </semantics>\n      </math></span> 相乘并加上偏置向量 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">b</mi>\n              <mn>2</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{b}_2</annotation>\n        </semantics>\n      </math></span>。权重矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi mathvariant=\"bold\">W</mi>\n              <mn>2</mn>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">\\mathbf{W}_2</annotation>\n        </semantics>\n      </math></span> 的维度是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n            <mo>×</mo>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff} \\times d_{model}</annotation>\n        </semantics>\n      </math></span>，这一步将中间层的输出映射回原始的输入维度 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span>。计算表示为：\n    <ul>\n      <li><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n            <semantics>\n              <mrow>\n                <mi mathvariant=\"bold\">y</mi>\n                <mo>=</mo>\n                <mtext>Linear</mtext>\n                <mo stretchy=\"false\">(</mo>\n                <msub>\n                  <mi mathvariant=\"bold\">z</mi>\n                  <mn>2</mn>\n                </msub>\n                <mo stretchy=\"false\">)</mo>\n                <mo>=</mo>\n                <msub>\n                  <mi mathvariant=\"bold\">z</mi>\n                  <mn>2</mn>\n                </msub>\n                <msub>\n                  <mi mathvariant=\"bold\">W</mi>\n                  <mn>2</mn>\n                </msub>\n                <mo>+</mo>\n                <msub>\n                  <mi mathvariant=\"bold\">b</mi>\n                  <mn>2</mn>\n                </msub>\n              </mrow>\n              <annotation encoding=\"application/x-tex\">\\mathbf{y}=\\text{Linear}(\\mathbf{z}_2)=\\mathbf{z}_2\\mathbf{W}_2+\\mathbf{b}_2</annotation>\n            </semantics>\n          </math></span></li>\n    </ul>\n  </li>\n</ol>\n<h3>先提升维度后缩小维度的原因</h3>\n<ol>\n  <li><strong>增加模型的表达能力</strong>：在第一层全连接层将维度从 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span> 提升到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n        </semantics>\n      </math></span>（<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n            <mo>></mo>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}>d_{model}</annotation>\n        </semantics>\n      </math></span>），可以让模型学习到更丰富的特征表示。更高的维度意味着网络有更多的自由度来捕捉输入数据中的复杂模式和关系。通过增加神经元的数量，模型可以表示更复杂的函数，从而提高其对不同输入的拟合能力。</li>\n  <li><strong>引入非线性</strong>：在提升维度后应用ReLU激活函数，ReLU是非线性函数，它可以打破线性关系，使得模型能够学习到非线性的特征。如果没有维度的提升，直接应用ReLU激活函数可能无法充分发挥其作用，因为低维度空间中的非线性变换可能不够丰富。</li>\n  <li><strong>信息整合与筛选</strong>：在提升维度并经过非线性变换后，第二层全连接层将维度从 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n        </semantics>\n      </math></span> 缩小回 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span>。这一步可以看作是对前面学习到的丰富特征进行筛选和整合，去除一些冗余信息，只保留与最终输出相关的重要特征。这样可以将高维空间中学习到的复杂信息压缩到原始的输入维度，以便后续的处理和与其他模块进行交互。</li>\n  <li><strong>与多头注意力机制互补</strong>：Transformer中的多头注意力机制主要关注序列中不同位置之间的依赖关系，而前馈神经网络则侧重于对每个位置的特征进行独立的非线性变换。先提升维度后缩小维度的结构可以与多头注意力机制相互补充，使得模型在捕捉序列的全局依赖关系的同时，也能对每个位置的特征进行有效的处理和转换。</li>\n</ol>",
    "type": 6,
    "level": 2,
    "freq": 0.0008309098,
    "analysis": "<h3>1. 题目核心</h3>\n<ul>\n  <li><strong>问题</strong>：Transformer前馈神经网络的结构是怎样的，为何先提升维度后缩小维度。</li>\n  <li><strong>考察点</strong>：\n    <ul>\n      <li>对Transformer前馈神经网络结构的掌握。</li>\n      <li>理解前馈神经网络中维度变化的原因。</li>\n    </ul>\n  </li>\n</ul>\n<h3>2. 背景知识</h3>\n<h4>（1）Transformer架构</h4>\n<p>Transformer是一种基于注意力机制的深度学习模型，前馈神经网络是其编码器和解码器中的重要组成部分，位于多头注意力机制之后。</p>\n<h4>（2）前馈神经网络基础</h4>\n<p>前馈神经网络是一种多层感知机（MLP），由输入层、隐藏层和输出层组成，信息从输入层单向传播到输出层。</p>\n<h3>3. 解析</h3>\n<h4>（1）Transformer前馈神经网络的结构</h4>\n<p>Transformer前馈神经网络通常由两个线性层（全连接层）和一个非线性激活函数组成。具体结构如下：</p>\n<ul>\n  <li>第一个线性层：将输入的特征向量维度从 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span> 提升到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n        </semantics>\n      </math></span>（<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n            <mo>></mo>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff} > d_{model}</annotation>\n        </semantics>\n      </math></span>）。</li>\n  <li>非线性激活函数：通常使用ReLU（Rectified Linear Unit）函数，对第一个线性层的输出进行非线性变换。</li>\n  <li>第二个线性层：将经过激活函数处理后的特征向量维度从 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>f</mi>\n                <mi>f</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n        </semantics>\n      </math></span> 缩小回 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n        <semantics>\n          <mrow>\n            <msub>\n              <mi>d</mi>\n              <mrow>\n                <mi>m</mi>\n                <mi>o</mi>\n                <mi>d</mi>\n                <mi>e</mi>\n                <mi>l</mi>\n              </mrow>\n            </msub>\n          </mrow>\n          <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n        </semantics>\n      </math></span>。</li>\n</ul>\n<h4>（2）先提升维度后缩小维度的原因</h4>\n<ul>\n  <li><strong>增加模型的表达能力</strong>：提升维度可以让模型在更高维的空间中学习特征，增加模型的复杂度和表达能力。在高维空间中，模型可以捕捉到更多的特征信息和特征之间的复杂关系，从而更好地拟合数据。</li>\n  <li><strong>引入非线性</strong>：通过提升维度和使用非线性激活函数，模型可以学习到更复杂的非线性映射。非线性激活函数可以打破线性关系，使模型能够处理更复杂的模式和数据分布。</li>\n  <li><strong>信息整合与筛选</strong>：在高维空间中学习到的特征信息经过第二个线性层的降维操作，可以筛选出最重要的特征信息，去除冗余信息，从而得到更精炼的特征表示。</li>\n</ul>\n<h3>4. 示例代码（使用PyTorch实现）</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.w_2(self.relu(self.w_1(x)))\n\n# 示例使用\nd_model = 512\nd_ff = 2048\ninput_tensor = torch.randn(10, 20, d_model)  # 输入张量\nfeed_forward = PositionwiseFeedForward(d_model, d_ff)\noutput = feed_forward(input_tensor)\nprint(output.shape)  # 输出形状应为 (10, 20, d_model)\n</code></pre>\n<h3>5. 常见误区</h3>\n<h4>（1）忽略维度变化的作用</h4>\n<ul>\n  <li>误区：只关注前馈神经网络的结构，而忽略了维度变化对模型性能的影响。</li>\n  <li>纠正：理解维度变化是为了增加模型的表达能力、引入非线性和筛选重要信息。</li>\n</ul>\n<h4>（2）错误理解激活函数的作用</h4>\n<ul>\n  <li>误区：认为激活函数只是简单的非线性变换，没有认识到它在维度变化过程中的重要性。</li>\n  <li>纠正：激活函数在提升维度后引入非线性，使模型能够学习到更复杂的模式。</li>\n</ul>\n<h4>（3）混淆维度变化与注意力机制的作用</h4>\n<ul>\n  <li>误区：将维度变化的作用与注意力机制的作用混淆，认为它们的功能相同。</li>\n  <li>纠正：注意力机制主要用于捕捉序列中元素之间的依赖关系，而维度变化是为了增强模型的表达能力和筛选特征。</li>\n</ul>\n<h3>6. 总结回答</h3>\n<p>Transformer前馈神经网络由两个线性层和一个非线性激活函数组成。第一个线性层将输入特征向量维度从 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mrow>\n              <mi>m</mi>\n              <mi>o</mi>\n              <mi>d</mi>\n              <mi>e</mi>\n              <mi>l</mi>\n            </mrow>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n      </semantics>\n    </math></span> 提升到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mrow>\n              <mi>f</mi>\n              <mi>f</mi>\n            </mrow>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n      </semantics>\n    </math></span>，接着使用ReLU激活函数进行非线性变换，最后第二个线性层将维度从 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mrow>\n              <mi>f</mi>\n              <mi>f</mi>\n            </mrow>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_{ff}</annotation>\n      </semantics>\n    </math></span> 缩小回 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n      <semantics>\n        <mrow>\n          <msub>\n            <mi>d</mi>\n            <mrow>\n              <mi>m</mi>\n              <mi>o</mi>\n              <mi>d</mi>\n              <mi>e</mi>\n              <mi>l</mi>\n            </mrow>\n          </msub>\n        </mrow>\n        <annotation encoding=\"application/x-tex\">d_{model}</annotation>\n      </semantics>\n    </math></span>。</p>\n<p>先提升维度后缩小维度的原因主要有：一是增加模型的表达能力，让模型在高维空间中捕捉更多特征信息和复杂关系；二是通过非线性激活函数引入非线性，使模型能够学习更复杂的映射；三是在高维空间学习后进行降维，筛选出重要特征信息，去除冗余信息，得到更精炼的特征表示。</p>",
    "more_ask": "<ol>\n  <li>\n    <p>\n      前馈神经网络中提升维度和缩小维度所使用的激活函数有什么特点，为什么选择这些激活函数？\n      提示：思考常见激活函数如ReLU、GELU等的特性，以及它们对模型性能和计算效率的影响。\n    </p>\n  </li>\n  <li>\n    <p>\n      在不同的自然语言处理任务中，前馈神经网络提升和缩小维度的比例是否会有差异，如何确定合适的比例？\n      提示：考虑不同任务（如文本分类、机器翻译等）的特点和数据规模，以及比例对模型复杂度和性能的影响。\n    </p>\n  </li>\n  <li>\n    <p>\n      前馈神经网络先提升维度后缩小维度的操作，对模型的泛化能力有什么影响？\n      提示：从模型的复杂度、过拟合和欠拟合的角度去分析对泛化能力的作用。\n    </p>\n  </li>\n  <li>\n    <p>\n      若在训练过程中改变前馈神经网络提升和缩小维度的参数，模型的收敛速度会发生怎样的变化？\n      提示：结合学习率、梯度更新等训练相关知识，思考参数改变对收敛速度的影响。\n    </p>\n  </li>\n  <li>\n    <p>\n      与不进行维度提升和缩小，直接进行线性变换相比，先提升维度后缩小维度的优势体现在哪些方面？\n      提示：从模型的表达能力、特征提取能力等方面进行对比分析。\n    </p>\n  </li>\n</ol>",
    "mindmap": "mindmap\n  root((Transformer前馈神经网络))\n    结构\n      第一层全连接层\n        线性变换\n        维度关系\n      激活函数\n        ReLU函数\n      第二层全连接层\n        线性变换\n        维度映射\n    先提升维度后缩小维度的原因\n      增加模型表达能力\n      引入非线性\n      信息整合与筛选\n      与多头注意力机制互补",
    "keynote": "结构：\n- 第一层全连接层：输入向量线性变换，维度从 $d_{model}$ 到 $d_{ff}$\n- 激活函数：ReLU函数\n- 第二层全连接层：中间层输出线性变换，维度从 $d_{ff}$ 到 $d_{model}$\n\n先提升维度后缩小维度原因：\n- 增加模型表达能力\n- 引入非线性\n- 信息整合与筛选\n- 与多头注意力机制互补",
    "group_id": 108,
    "kps": [
      "深度学习",
      "神经网络"
    ],
    "years": [
      2024,
      2023
    ],
    "corps": [
      "字节跳动",
      "美团"
    ]
  }
}